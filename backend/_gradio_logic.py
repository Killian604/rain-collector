
from backend import shared, vllm_util
import gradio as gr
from transformers import AutoTokenizer, AutoFeatureExtractor, set_seed
import numpy as np
import torch
from threading import Thread
from typing import List, Tuple
from pydub import AudioSegment
import io
from scipy.io import wavfile
device = "cuda:0" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
torch_dtype = torch.float16 if device != "cpu" else torch.float32

repo_id = "parler-tts/parler_tts_mini_v0.1"

# jenny_repo_id = "ylacombe/parler-tts-mini-jenny-30H"
# from streamer import ParlerTTSStreamer
#
# model = ParlerTTSForConditionalGeneration.from_pretrained(
#     jenny_repo_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True
# ).to(device)
#
# tokenizer = AutoTokenizer.from_pretrained(repo_id)
# feature_extractor = AutoFeatureExtractor.from_pretrained(repo_id)
#
# sampling_rate = model.audio_encoder.config.sampling_rate
# frame_rate = model.audio_encoder.config.frame_rate


def read_response(prompt: str):

    play_steps_in_s = 2.0
    play_steps = int(frame_rate * play_steps_in_s)

    description = "Jenny speaks at an average pace with a calm delivery in a very confined sounding environment with clear audio quality."
    description_tokens = tokenizer(description, return_tensors="pt").to(device)

    streamer = ParlerTTSStreamer(model, device=device, play_steps=play_steps)
    prompt = tokenizer(prompt, return_tensors="pt").to(device)

    generation_kwargs = dict(
        input_ids=description_tokens.input_ids,
        prompt_input_ids=prompt.input_ids,
        streamer=streamer,
        do_sample=True,
        temperature=1.0,
        min_new_tokens=10,
    )

    set_seed(42)
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    for new_audio in streamer:
        print(f"Sample of length: {round(new_audio.shape[0] / sampling_rate, 2)} seconds")
        yield prompt, numpy_to_mp3(new_audio, sampling_rate=sampling_rate)

